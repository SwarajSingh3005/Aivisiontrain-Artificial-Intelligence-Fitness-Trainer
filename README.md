# 1.Abstract

Integrating artificial intelligence (AI) with fitness training has revolutionized how individuals approach exercise routines. This thesis delves into the creation of Aivisiontrain, an AI-powered fitness trainer that addresses fitness-related inquiries and monitors real-time exercise performance. The implementation involves utilizing Python code to build a chatbot interfaced with OpenAI API for fitness inquiries and employing MediaPipe's library to monitor exercise execution through webcam analysis. The system is further extended to provide feedback and guidance during workout sessions. The Aivisiontrain potential for improving exercise accuracy, providing personalized routines, and promoting healthier lifestyles is explored.

# 2.Video Demo

https://github.com/SwarajSingh3005/Aivisiontrain-Artificial-Intelligence-Fitness-Trainer/assets/114939556/31f8fd7f-eb8d-4b31-acc0-dc37eacca13a


# 3.Methodology

3.1. Pose Estimation

I employed the MediaPipe (Mediapipe 2023) framework for human pose estimation to extract critical information regarding the user's posture. MediaPipe provides a sophisticated approach to detecting key body landmarks. When the user's input is processed through the MediaPipe library, it discerns the significant keypoints on the body, presenting the results as coordinates along the X, Y, and Z axes for 33 primary landmarks. These pinpointed locations elucidate the positioning of major body parts within the provided image. By leveraging these keypoints, it's feasible to recreate an intricate representation of the user's skeletal structure.
For a clearer understanding, refer to Figure 1 where these landmarks, often known as "landmarks," represent major junctions and regions on a human body. These are systematically indexed from 0 through 32, summing up to 33 distinct landmarks. For facial features, landmarks in the range of 0 to 10 are utilized. Landmarks spanning from 11 to 22 are designated for the upper body recognition, encapsulating areas like shoulders, wrists, elbows, and hands. The concluding set of landmarks, from 23 to 32, pinpoints the lower body, comprising of regions such as the hips, knees, legs, and feet.

MediaPipe, an open-source and multi-platform library, delivers tailor-made machine learning solutions optimized for real-time streaming media—this encompasses a spectrum of media from audio, videos, to sequential data. Compatible across a wide array of platforms, including iOS, Python, JavaScript, and Android, MediaPipe offers a plethora of tools and solutions. Its repertoire consists of features like pose estimation, hair segmentation, face mesh, and motion tracking.
The data that is fed into the MediaPipe library primarily originates from real-time camera captures. Post processing, the library yields a comprehensive list of corresponding keypoints in the Cartesian coordinates (X, Y, and Z). Harnessing these coordinates, one can glean a fairly accurate depiction of the human body's structure and orientation within any given image or live video feed. As stated in MediaPipe's (Mediapipe 2023) official documentation, it operates at a commendable frame rate of 30 frames per second.

![Mediapipe](https://github.com/SwarajSingh3005/Aivisiontrain-Artificial-Intelligence-Fitness-Trainer/assets/114939556/0565b48e-0a47-42fb-9bc5-337b72a2637b)


3.2. Chatbot

I utilized the OpenAI API(OpenAI 2023) for my AI fitness chatbot, which permits developers to integrate the prowess of models like GPT-3 and GPT-4 into their platforms. The API facilitates communication with OpenAI-hosted models without the need to operate them on personal infrastructure. To interact with it, you typically use HTTP requests, authenticating with API keys provided by OpenAI.
I accessed the API through the OpenAI Playground, a platform crafted for users to experiment with and understand the API's workings. It's a versatile space, predominantly for tech R&D, permitting users to engage with machine learning models. With it, developers can test-drive models like GPT-3 and GPT-4,honing in on functionalities such as text generation and image classification. Figure 2, depicts the process of integrating OpenAI code from the Playground into a personal project; with the provided code and API key, anyone can seamlessly incorporate OpenAI functionalities.Discussing the models, GPT-3's "davinci" variant is built on the Transformer framework that uses attention mechanisms to discern input sequences' relevance. The GPT-3 model, packed with 175 billion parameters, stands as one of the grandest in the commercial domain. GPT-3 is trained on extensive web-based text, acquainting itself with grammar, facts, logic, and elements of general wisdom. It's further refined using specific data, often with human oversight, in line with OpenAI's guidelines (Iffort 2023).
For my chatbot, I employed text-davinci-003, with all responses powered by the "davinci" variant. Among GPT-3's offerings, text-davinci-003 is the frontrunner, surpassing models like text-curie-001 and text-babbage-001. It excels in generating refined, extended content, outperforming its precursor, Text-Davinci-002. Specifically crafted for tasks requiring adherence to instructions, text-davinci-003 is adept at producing precise, succinct outputs, even when presented with unfamiliar prompts (Openai 2023).

![Playground - OpenAI API](https://github.com/SwarajSingh3005/Aivisiontrain-Artificial-Intelligence-Fitness-Trainer/assets/114939556/1790893a-7851-4189-8bb3-1fe32319d1e4)


3.3. Computer Vision

OpenCV, an acronym for Open-Source Computer Vision Library, is a renowned library dedicated to real-time computer vision functions. It’s coded in a diverse set of languages like C, C++, Python, Java, and assembly. To bolster real-time capabilities, GPU acceleration was embedded into OpenCV from 2011 onwards. With a repertoire of over 2500 optimized algorithms, OpenCV stands as a comprehensive suite for both classical and cutting-edge computer vision and machine learning techniques. This encompasses features like face and object recognition, camera motion tracking, image merging, and much more. Tailored for real-time applications, OpenCV leverages MMX and SSE instructions when they're accessible. (OpenCV 2021)


# 4.Implementation

4.1. Building the Fitness Inquiry Chatbot

4.1.1. Speech recognition
Using SpeechRecognition (Python 2018), I've been able to harness the capability of modern computational systems to transform spoken language into text. First, when someone speaks into the device's microphone, that speech is picked up by the pyaudio library, which serves as a bridge to access audio data from audio input devices on the system. The audio data captured is then passed to the SpeechRecognition library, which, with the help of pyaudio, extracts the sound from the microphone. This raw audio data is sent to Google's Web Speech API (Google 2023) through the method recognize_google(). Powered by sophisticated machine learning algorithms, the API processes the audio, transcribing the speech into text. Once transcribed, the resulting text is received by the software I've developed, which then processes or acts based on what the user initially said.

4.1.2. User Experience
For an enhanced user experience, the Python script is crafted to prioritize usability and user satisfaction. Always on the lookout for instructions, the system springs to action when the user says "wake up", prepping the assistant to attend to the user's demands. Central to its functionality is the main_task_execution() function. This employs the SpeechRecognition library to convert voice prompts into text. To address accidental activations from previous versions, a distinct cue, "fitness", was introduced. Earlier, the system sometimes mistook unrelated chats for commands, giving unrelated responses. Now, only when "fitness" is recognized does the robust main_task_execution() function activate, processing a range of commands. From launching and closing apps, activating camera feed via OpenCV, to playing music from specific folders, it’s adept at multitasking. Users can also explore web commands, visiting popular platforms like YouTube and GitHub or fetching their IP address. The "bye" command offers a smooth exit, with the system expressing gratitude before awaiting its next use.

4.1.3. OpenAI API
The script artfully merges voice-recognition technology with OpenAI's GPT model, with the goal of creating a specialized voice assistant for fitness inquiries. At the heart of this system is the chat_with_openai() function. This enables an interactive dialogue where users can voice their questions. Once a query is posed, the system consults the OpenAI API to fetch pertinent answers, then vocally delivers this information back, ensuring fluid information exchange.However, it was noted that the default OpenAI API responses were lengthy and tedious to listen to. To address this, the maximum token number was trimmed from its default of 256 down to 10. Moreover, the model's randomness hyperparameter was tweaked to match the desired chatbot output. A higher setting yields more unpredictable results, while a lower one offers more consistent replies.

A significant component is the is_fitness_keyword_present() function. It serves to examine user queries for fitness-specific terms. If such terms are detected, the system precisely tunes its responses to the fitness realm. Conversely, if a fitness keyword isn't found, or if a user inquiry about something outside the scope of fitness, the assistant will respond that it lacks information on the subject.
Enhancing the overall user experience is the generate_ai_response() function. If a user's query incorporates the term "make", this function prompts a response from the OpenAI API. The system then stores this response in a text file located in an "Openai" directory, highlighting the system's trust in the OpenAI API for intricate query solutions.
In summary, this meticulously designed script is a testament to the vast potential of OpenAI's API. By seamlessly blending cutting-edge voice technology with the robust capabilities of the GPT model, it signals the emergence of specialized voice assistants. In this case, the assistant offers specialized guidance in the comprehensive realm of fitness.


4.2. Real-time Exercise Monitoring and Counting
In our detailed examination of computer vision and exercise monitoring, we developed four unique Python scripts: PoseModule5, Bicep_Curls_Exercise, Bicep_Curls_Exercise, and Bicep_Curls_Exercise. The posemodule script primarily focuses on estimating human posture, while the other three are dedicated to monitoring specific exercises like bicep curls, jumping jacks, and squats. All these scripts harness the 'blazepose' functionality from the Mediapipe framework for pose estimation, seamlessly integrating with the OpenCV library to enable real-time video streaming from a webcam.
Inside posemodule.py, there are crucial class methods. The get_pose method processes an input image by converting it to RGB, identifying pose landmarks, and optionally annotating these landmarks with their connections on the image while displaying detection confidence. In contrast, the get_landmark_position method predicts all 33 keypoints, each with 3 attributes: x and y coordinates and visibility. Additionally, it considers two virtual alignments. (Figure 3) shows the landmarks with distinct circles, where attributes like color and size indicate the detection confidence of each landmark.
Adding to the script's functionality, it can calculate the frame rate (FPS) by measuring the time differences between consecutive frames. This comes with a handy overlay feature that displays essential metrics on the live video, such as the real-time FPS.

![Video_demo mp4 14-08-2023 9 38 09 AM](https://github.com/SwarajSingh3005/Aivisiontrain-Artificial-Intelligence-Fitness-Trainer/assets/114939556/975ed0cb-2374-435c-8deb-e82081b426ed)

4.2.1. Calculating Angles

Initially, we determine the coordinates of the three essential joints to compute the angle. Using NumPy, we then deduce the slopes of these joints. While angles are primarily computed in radians, they can easily be converted into degrees.
The formula Angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))
allows us to find the angle between two lines directly without the need to determine the slope. Depending on the joints selected, this method discerns the inter-joint angle. For my left bicep curl application, I focused on the elbow angle, as it's a critical indicator of the correct form for a left bicep curl.

4.2.2. Repetition Counting
To count the repetitions, the algorithm monitors the probability of a target pose class. When the probability of the “down” pose class passes a certain threshold for the first time, counter increases by one. Then successful transition of hand from down or up increases counter. Given the landmarks are detected, in the case of bicep curls script calculates the angle of the right arm using specific landmarks denoting the shoulder, elbow, and wrist. This angle is further interpolated to determine its percentage representation, which is instrumental in identifying the stage of the dumbbell curl. A bar visualization is rendered on the image to depict this percentage, with its height and color indicating the extent and stage of the curl respectively. A full curl is denoted by the angle reaching 100%, resulting in the color turning green and incrementing the repetition count (Figure 4.1). When the arm returns to the starting position (angle at 0%), the count is once again incremented, thus completing one full repetition.


![Video_demo mp4 14-08-2023 9 34 59 AM](https://github.com/SwarajSingh3005/Aivisiontrain-Artificial-Intelligence-Fitness-Trainer/assets/114939556/59468ef1-5c50-491c-982f-345f705b7578)
![Video_demo mp4 14-08-2023 9 35 42 AM](https://github.com/SwarajSingh3005/Aivisiontrain-Artificial-Intelligence-Fitness-Trainer/assets/114939556/39af5918-ad6b-45a3-9913-42619da1e16d)

4.2.3. Bicep curls
Bicep curls (Rogers 2007), sometimes referred to as arm curls or dumbbell curls, are a cornerstone exercise in weight training, targeting the biceps in the upper arm. The exercise is performed by standing upright with dumbbells in hand, palms facing outwards. Keeping the upper arms anchored to the side,the weights are raised by flexing the elbow, bringing the dumbbells towards the shoulders. In a full curl, the dumbbells might reach the level of the eyes or even the forehead. While the primary muscle worked is the biceps of the upper arm, the exercise also engages the brachialis and brachioradialis in the lower arm.
When the script runs, it activates the default camera, streaming live video and leveraging the PoseModule3 for pose detection. Within a consistent loop, the script captures individual frames, adjusting them to a consistent size of 1280x720 pixels. The pose detection mechanism then processes these frames to identify specific pose landmarks, which correspond to distinct anatomical points on the body.
In the case of bicep curls, the code intends to calculate the angle of the right arm, focusing on the elbow joint. The method calculate_angle of the pose_detector is then invoked with the image and landmarks of points 12, 14, and 16, these points represent the right shoulder, right elbow, and right wrist respectively.
The dynamics of the elbow angle during a bicep curl are central to the tracking process. As one engages in the exercise, the elbow's angle starts more open when the arm is straight and narrows as the arm curls. This fluctuating angle provides insights into the phase of the curl, facilitating the counting of repetitions. Rep counts are deduced from the angle's degree of closure and the direction of arm movement, whether upward or downward.

4.2.4. Squat
A squat is a core exercise that involves standing upright, then lowering the hips and returning to the starting position. During the descent, the hip and knee joints bend while the ankle moves upward; conversely, as one rises, these joints straighten, and the ankle moves downward. This movement engages primarily the quadriceps, adductor magnus, and gluteus maximus muscles. Moreover, the erector spinae and abdominal muscles provide stabilizing support. Squats are essential for developing lower body muscle endurance and mass and fortifying core stability. They're pivotal in powerlifting and numerous workout regimens(Ribeiro 2023).
The functions, get_right_leg_angle and get_left_leg_angle, procure body reference points using the get_landmark_positions function. These reference points, termed landmarks here, are distinct body markers recognized by the pose detector. For the right leg, landmarks 24, 26, and 28 correspond to the hip, knee, and ankle respectively.
After obtaining the landmarks, the angles of both legs are deduced with the calculate_angle function. These angles are imperative to determine the squat's phase (either rising or descending). In the process_image function, the angles from both legs are combined to deduce the cumulative squat angle, which helps in rectifying any disparity or imbalance in the user's motion. This combined angle is subsequently relayed to the draw_workout_info function.The draw_workout_info function showcases the squat's progress numerically and through a visual bar, using the average leg angle to gauge this progression. Repetitions are also monitored by observing angle directional changes. If the mean angle indicates a full standing posture and the last movement was descending, it counts as half a repetition. Likewise, transitioning from standing to a squatting stance also accounts for half a repetition. Therefore, a complete squat (down then up) equals one full repetition, added in 0.5 increments.
The get_bar_color function modifies the progress bar's hue to green during the squat's peak points (fully upright or deep squat) and alters the direction indicator. For any in-between angles, the bar appears red. For each video frame, the pose is recognized, leg angles are determined, repetitions are counted, and the related data is superimposed on the video, ready for display.

4.2.5. Jumping Jacks
Jumping jacks are a classic full-body workout enhancing cardiovascular health. Starting with feet close and arms down, you jump and spread your feet past shoulder-width, raising arms overhead. This action reverses with another jump to the original pose. This exercise increases heart rate, fosters cardiovascular stamina, and works the legs, core, and arms. The synchronized limb movement improves coordination. As no special gear is needed, it's a handy workout for various locales. Those new to this or with health issues should be careful and might seek medical advice for safety (Baharuddin 2023).
The get_hand_position() method retrieves the vertical coordinates of both wrists, essential for gauging hand positions during the exercise. The get_ankle_distance() method calculates the gap between the ankles, indicating if the legs are spread or together.
The core logic for repetition counting is in the detect_jump_repetition() method. This method checks both wrist positions and ankle spacing to identify the jumping jack's stage. If ankle separation surpasses 150 (showing legs are wide) and both wrists' vertical positions are below 300 (hands raised), it marks the jumping jack's initial phase. In contrast, if ankles are closer than 100 (legs close) and both wrist positions exceed 400 (hands lowered), the jumping jack concludes one round, increasing the repetition tally.
Lastly, the draw_workout_info() method superimposes the exercise's details onto the image. This includes an ankle gap visual, total repetition number, and cues on the current jumping jack phase ("Jump!" or "Keep Going!"). The system leverages body pointers, like wrist vertical positions and ankle spacing, to recognize the exercise's stages and keep a repetition tally.



4.3. How did combined OpenAi and Mediapose
Using voice commands to run and then halt a Python file was a challenge I faced. After deep investigation, I stumbled upon the notion of Process ID (PID) within the Windows environment (DOMARS 2023). Leveraging this knowledge, I developed the following solution:
The program listens for specific voice prompts like "Exercise One" for bicep curls, "Exercise Two" for squats, and "Exercise Three" for jumping jacks.
Upon hearing "Exercise One", and if no other exercise script is currently active, it launches the Python script at file location. The script's Process ID (PID) is then saved for future reference. If the exercise script is already active, the user is alerted with "Gym exercise is already open."
However, if the commands "close one", "close two", or "close three" are recognized and the associated exercise script is running, that process is ended. For example, the command "close one" prompts a user notification: "Closing Bicep curls exercise".
This solution provides an intuitive and effective way to manage a Python application using vocal cues.


# 5. Results and Discussion

5.1. Pose Estimation Accuracy
In the process of rigorously testing MediaPipe's pose detection for the right elbow landmark, we identified a clear correlation between detection accuracy and the subject's distance from the camera. When subjects were positioned at an optimal distance, the model exhibited a robust accuracy range of 90% to 95%(Figure5.1), reflecting its high reliability. In contrast, when subjects were too close to the camera, accuracy sharply declined to between 40% and 50%(Figure5.2). This significant variation underscores the model's sensitivity to the subject's positioning relative to the camera.
The min_detection_confidence parameter plays a crucial role in this process. It sets the baseline for pose detection, meaning that if it's set to 0.5, only poses the model has at least 50% confidence in will be recognized. In our tests, a confidence level of 0.5 was achieved at the optimal distance, while nearly no confidence was observed when too close It's essential for users to understand the model's confidence as an indication of its predictive reliability. However, it's equally crucial to understand that these confidence metrics aren't direct measures of accuracy. For an accurate assessment, a comparison between the model's predictions and ground truth labels on a benchmark dataset would be required. Future iterations of the model might prioritize enhanced consistency across varied distances and real-time feedback for users to adjust their positioning.

![Media Player Classic Home Cinema 14-08-2023 10 59 03 AM](https://github.com/SwarajSingh3005/Aivisiontrain-Artificial-Intelligence-Fitness-Trainer/assets/114939556/476313aa-8f53-4b16-b563-e44d5af9cb1c)


![Media Player Classic Home Cinema 14-08-2023 10 59 17 AM](https://github.com/SwarajSingh3005/Aivisiontrain-Artificial-Intelligence-Fitness-Trainer/assets/114939556/677bda79-ca5a-40ad-bda8-70b1ec0891b6)



Personal Observations:
1. Many participants spoke softly and had pronunciation variations, leading to the Aivisiontrain inability to comprehend their queries.
2. During exercises, certain participants stood too close to the camera, presumably to be closer to the microphone. As a solution, I've added a prompt in the code instructing users to step back three paces from the camera before beginning an exercise.
3. 3. I noted that the rep counter and pose estimator faced difficulties with taller users. For instance, a participant who was 6'5" had to stand considerably far from the camera to be detected.
4. Overall, participants seemed to prefer the exercise and rep counting features over the chatbot. The misinterpretations due to varied accents and pronunciations suggest that training the model for multiple accents might be beneficial.


# 6. Conclusion
In conclusion, the development of the AI-powered fitness trainer, leveraging Python code and the OpenAI API Playground for chatbot interactions and real-time exercise monitoring through Mediapipe, marks a pivotal stride in the convergence of cutting-edge technology and personalized fitness guidance. This innovative system's initial implementation has showcased its potential to adeptly address user queries, offer exercise assistance, and count repetitions. By seamlessly amalgamating AI and fitness training, it responds to the contemporary demand for tailored and interactive fitness experiences, fostering improved user engagement, motivation, and adherence. Beyond its immediate impact, this fusion holds implications for revolutionizing the fitness industry, potentially democratizing access to expert guidance and paving the way for individually curated workout plans. Looking ahead, identified avenues for enhancement—including broadening fitness knowledge, diversifying exercise options, implementing user-centric features, integrating AI-generated routines—underscore the promising trajectory of AI-powered fitness trainers. Ultimately, this marriage of technology and fitness not only augments workout experiences but also envisions a future where individuals achieve their fitness aspirations with unprecedented precision and empowerment, reshaping wellness and transforming lives.
